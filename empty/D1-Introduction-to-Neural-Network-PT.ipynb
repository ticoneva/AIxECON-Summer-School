{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Network (PyTorch)\n",
    "\n",
    "Version: 2024-8-6\n",
    "\n",
    "The widespread adoption of artificial intelligence in recent years has been largely driven by advancement in neural networks. \n",
    "\n",
    "Neural network is fundamentally numeric computation, so any software with decent numeric computation capabilities can be used to construct and train a neural network. That said, while in theory you can construct a neural network in Excel, in practice it will be very troublesome since Excel is not designed with neural network in mind. Modern neural network applications have consolidated around three platforms:\n",
    "- [Tensorflow](https://www.tensorflow.org/) from Google.\n",
    "- [Flax](https://github.com/google/flax), also from Google.\n",
    "- [PyTorch](http://pytorch.org/), originally from Meta but now managed by an independent foundation.\n",
    "\n",
    "At the lowest level, these platforms are essentially NumPy with the ability to run on GPUs. \n",
    "We do not want to write the basic building blocks of neural networks from scratch, however\n",
    "if we are just trying to learn how they work.\n",
    "Therefore in this course, we will focus on two types of components that build on top of these platforms:\n",
    "\n",
    "1. **High-level API for constructing neural network**: \n",
    "    [`keras`](https://keras.io/) of Tensorflow and and \n",
    "    [`nn.Module`](https://pytorch.org/docs/stable/nn.html) of PyTorch \n",
    "    provides ready-to-use building blocks for the construction of neural networks.\n",
    "2. **Libraries that provides access to fully-trained models**. \n",
    "    The most prominent examples are Hugging Face's [Transformers](https://huggingface.co/docs/transformers/index) \n",
    "    and [fastai](https://github.com/fastai/fastai).\n",
    "\n",
    "In this notebook we will use PyTorch, which is currently the platform of choice for research. \n",
    "There is a separate notebook on the same topic that uses Keras on Tensorflow instead.\n",
    "\n",
    "<img src=\"https://scrp.econ.cuhk.edu.hk/workshops/ai/images/nn_libraries_2024.png\" width=\"80%\">\n",
    "\n",
    "## A. PyTorch vs Keras-Tensorflow\n",
    "\n",
    "If you are familiar with how neural networks are constructed in Keras,\n",
    "there is no exact equivalent on PyTorch. \n",
    "The main differences are as follows:\n",
    "- The model structure is defined within a subclass of `torch.nn.Module`.\n",
    "- You have to specify&mdash;i.e. code&mdash;what happen during the forward pass.\n",
    "- Pure PyTorch also requires you to code the training loop, as well as what happens during\n",
    "    validation, testing and inference. These can be replaced by trainers from libraries \n",
    "    such as [pytorch-accelerated](https://pytorch-accelerated.readthedocs.io/en/latest/),\n",
    "  [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/),\n",
    "  and most recently, [Keras 3](https://keras.io/keras_3/).\n",
    "- Data needs to be manually placed in the right device. This can be automatically handled\n",
    "    by Hugging Face's [Accelerate](https://github.com/huggingface/accelerate) library.\n",
    "\n",
    "These differences make PyTorch a bit harder to use, but you gain more flexibility as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will first disable the server's GPU so that everything runs on its CPU. Later we will turn it back on to see how much speed up we can get. This setting has no effect if you do not have a (Nvidia) GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: Binary Neural Network Classifier\n",
    "\n",
    "As a first example, we will train a neural network to the following classification task:\n",
    "\n",
    "|$y$|$x_1$|$x_2$|\n",
    "|-|-|-|\n",
    "|0|1|2|\n",
    "|1|0|5|\n",
    "\n",
    "with $y$ being $1 - x_1$ and $x_2$ being just an irelevant random number.\n",
    "\n",
    "To be clear: there is absolutely no need to use neural network for such as simple task. A simpler model such as logit will train a lot faster and potentially with better accuracy.\n",
    "\n",
    "We first load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv(\"../data/D1-data-1.csv\")\n",
    "y = data['y']\n",
    "X = data[['x1','x2']]\n",
    "\n",
    "#Shuffle and split data into train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader\n",
    "\n",
    "Unlike Keras, NumPy arrays cannot be directly provided to PyTorch models. \n",
    "We will instead do the following:\n",
    "1. Create PyTorch tensors from NumPy arrays.\n",
    "2. Create a PyTorch Dataset from the tensors.\n",
    "3. Create a PyTorch DataLoader from the Dataset.\n",
    "\n",
    "Note that Tensorflow does have a similar data loading structure.\n",
    "It is just that Keras provides a simplier interface while PyTorch does not.\n",
    "\n",
    "Because we have to do this for every dataset we use, \n",
    "we will write a function the takes NumPy arrays and return a PyTorch Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def numpyToDataLoader(X,y,batch_size=32):\n",
    "    # Transform numpy array to torch tensor\n",
    "\n",
    "    # create datset and dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply this function to our train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = numpyToDataLoader(X_train,y_train)\n",
    "dl_test = numpyToDataLoader(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Neural Network\n",
    "\n",
    "We will construct a neural network classifier for this task. \n",
    "\n",
    "A neural network model is made up of multiple layers. The simpliest model would have three layers:\n",
    "- An *input layer*. This layer specify the nature of the input data. In this example, we only need to tell Keras that we have two variables to input.\n",
    "- A *hidden layer*. This layer contains neuron(s) that process the input data.\n",
    "- An *ouput layer*. The neurons in this layer process the output from the hidden layer and generate predictions. This layer contains as many neurons as the number of target variables we try to predict. \n",
    "\n",
    "Below is the simplest neural network one can come up with, with only one hidden neuron. The neuron computes the following function:\n",
    "}\n",
    "$$\n",
    "F \\left( b + \\sum\\nolimits_{i}{w_{i}x_{i}} \\right)\n",
    "$$\n",
    "\n",
    "where $x_i$ are inputs, b the intercept (called *bias* in machine learning), $w_i$ coefficients (called *weights*) and $F$ is an *activation function*. In this example we will use the logistic function (also called the *sigmoid function*) as the activation function:\n",
    "\n",
    "$$\n",
    "F(z) = \\frac{e^z}{1+e^z}\n",
    "$$\n",
    "\n",
    "So the neuron is essentially a logit regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, cuda, optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Your neural network needs to be implemented in a subclass\n",
    "# of torch.nn.Module\n",
    "\n",
    "\n",
    "# Create the model and transfer it to the chosen device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "PyTorch has no trainer build in, so we have to write a training loop that does the following:\n",
    "1. Loop through each epoch.\n",
    "2. Within each epoch, loop through each mini-batch.\n",
    "3. Within each mini-batch:\n",
    "    1. Compute loss.\n",
    "    2. Compute gradients.\n",
    "    3. Update parameters.\n",
    "\n",
    "The process requires us to specify the loss function and optimizer, which we will provide\n",
    "later. Additionally, we also need to write code to keep track of progress.\n",
    "\n",
    "Since we are going to train multiple models, we put the loop in a function so that we can \n",
    "reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epochs=5, quiet=False):\n",
    "    # Set the model into training mode\n",
    "    # Does not actually train the model\n",
    "\n",
    "\n",
    "    # Double loop: epoch - mini-batch\n",
    "        \n",
    "        # Create an empty list to store mini-batch loss in this epoch\n",
    "        \n",
    "        # Mini-batch loop\n",
    "        \n",
    "            # Transfer mini-batch to chosen device\n",
    "            \n",
    "\n",
    "            # Compute prediction error\n",
    "           \n",
    "\n",
    "            # Backpropagation\n",
    "              # Reset gradients to zero\n",
    "              # Compute the gradients\n",
    "              # Update parameters based on the chosen optimizer\n",
    "\n",
    "            # Append loss to loss list\n",
    "            \n",
    "\n",
    "        if not quiet:\n",
    "            # Display overall loss\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model by calling the `train` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function, optimizer and number of epochs\n",
    "\n",
    "\n",
    "# Start training\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "To evaluate the model, we have a loop similar to the training loop,\n",
    "but without going through multiple epochs and without updating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Transfer mini-batch to chosen device\n",
    "        Xb = X.to(device)\n",
    "        yb = y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(Xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        # Append loss to loss list\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    # Overall loss\n",
    "    loss_overall = np.mean(loss_list)\n",
    "    return loss_overall\n",
    "    \n",
    "test(dl_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike OLS, a neural network's performance could vary across runs. Run the code a few more times and see how the performance vary.\n",
    "\n",
    "### Inference\n",
    "\n",
    "We can make prediction (this is called *inference* in machine learning) with yet another loop,\n",
    "this time without computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "x = np.array([[0,1]])\n",
    "\n",
    "# Set up tensor, dataset and datalaoder\n",
    "\n",
    "\n",
    "# List to save prediction\n",
    "\n",
    "\n",
    "# Inference loop\n",
    "\n",
    "    # Transfer mini-batch to chosen device\n",
    "    \n",
    "    \n",
    "    # Compute prediction\n",
    "    \n",
    "\n",
    "# The combined array of predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "Training neural network models are time consuming, so we usually want to save \n",
    "trained models for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# Load\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Activations\n",
    "\n",
    "Different activation can have profound impact on model performance. Besides ```nn.Sigmoid```, which is just a different name for the logistic function, there are other activation function such as ```nn,Tanh``` and ```nn.ReLU```. *ReLU*, which stands for **RE**ctified **L**inear **U**nit, is a particular common choice due to its good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ReLU instead of sigmoid in the hidden layer\n",
    "\n",
    "\n",
    "# Create the model and transfer it to the chosen device\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=10)\n",
    "test(dl_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is ReLU preferred over the logistic function? Let us take a look at the shape of each function:\n",
    "\n",
    "<img src=\"https://scrp.econ.cuhk.edu.hk/workshops/ai/images/logistic_v_relu.png\">\n",
    "\n",
    "The most prominent feature of the logistic function is that it is bounded between 0 and 1. This means it is virtually flat for very large or very small input values, and flat means small gradient. As gradient descent relies on gradient to learn, small gradient implies slow learning. ReLU avoids this issue by being linear above zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Dropout\n",
    "\n",
    "As neural networks are highly flexible, they can easily overfit. Dropout is a regularization technique that works by randomly setting the outputs of some neurons to zero, thereby forcing the network to not rely too much on a specific neurons or feature. The function below added a 50% dropout to the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Neural Network Regression\n",
    "\n",
    "Next we are going use a neural network in a regression task. The true data generating process (DGP) is as follows:\n",
    "\n",
    "$$\n",
    "y = x^5 -2x^3 + 6x^2 + 10x - 5\n",
    "$$\n",
    "\n",
    "The model does not know the true DGP, so it needs to figure out the relationship between $y$ and $x$ from the data.\n",
    "\n",
    "First we generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 1000 samples\n",
    "X = np.random.rand(1000,1)\n",
    "y = X**5 - 2*X**3 + 6*X**2 + 10*X - 5\n",
    "\n",
    "#Shuffle and split data into train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "dl_train = numpyToDataLoader(X_train,y_train)\n",
    "dl_test = numpyToDataLoader(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer with 100 neurons\n",
    "\n",
    "        \n",
    "# Create the model and transfer it to the chosen device\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=20)\n",
    "test(dl_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Model Construct Modifiable\n",
    "\n",
    "We can add arguments to the `__init__` method of our subclass of `nn.Module`, \n",
    "allowing us to create models with different settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer with variable hidden neurons and activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of code repetition outside of the subclass.\n",
    "We will enclose them in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def polyNN(data, \n",
    "           epochs=200, \n",
    "           batch_size=32,\n",
    "           **kwargs):\n",
    "    \n",
    "    # Record the start time\n",
    "    start = time.time()    \n",
    "    \n",
    "    # Unpack the data\n",
    "    X_train, X_test, y_train, y_test = data    \n",
    "\n",
    "    # Convert data to PyTorch tensor\n",
    "    dl_train = numpyToDataLoader(X_train,y_train,batch_size=batch_size)\n",
    "    dl_test = numpyToDataLoader(X_test,y_test,batch_size=batch_size)    \n",
    "    \n",
    "    # Create the model and transfer it to the chosen device\n",
    "    model = NNReg(**kwargs).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Train and the model\n",
    "    train(dl_train, model, loss_fn, optimizer, epochs, quiet=True)\n",
    "    \n",
    "    # Collect and display info\n",
    "    loss_tr = round(test(dl_train, model, loss_fn),4)\n",
    "    loss_te = round(test(dl_test, model, loss_fn),4)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    elapsed = round(time.time() - start,2)  \n",
    "    \n",
    "    print(\"Hidden count:\",str(kwargs['hidden_count']).ljust(5),\n",
    "          \"Parameters:\",str(param_count).ljust(6),\n",
    "          \"loss (train,test):\",str(loss_tr).ljust(7),str(loss_te).ljust(7),\n",
    "          \"Time:\",str(elapsed)+\"s\",\n",
    "         )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily try out different settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_test_split(X,y)\n",
    "\n",
    "polyNN(data,hidden_count=1)\n",
    "polyNN(data,hidden_count=10)\n",
    "polyNN(data,hidden_count=50)\n",
    "polyNN(data,hidden_count=100)\n",
    "polyNN(data,hidden_count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the universal approximation theorem in work: the more neurons we have the better the fit.\n",
    "\n",
    "One trick that can often improve performance: *standardizing* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scalar = preprocessing.StandardScaler().fit(X)\n",
    "X_std = scalar.transform(X)\n",
    "\n",
    "data_std = train_test_split(X_std,y)\n",
    "\n",
    "polyNN(data_std,hidden_count=1)\n",
    "polyNN(data_std,hidden_count=10)\n",
    "polyNN(data_std,hidden_count=50)\n",
    "polyNN(data_std,hidden_count=100)\n",
    "polyNN(data_std,hidden_count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `StandardScaler` works quite well when there is only a single feature, its sensitivity to outliers makes it unsuitable for situations with mulitple highly unbalanced features. Scikit-learn offers <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\">other scalers</a> such as `RobustScaler` that might work better in those cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer with variable hidden neurons and activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyNN(data_std,hidden_count=1)\n",
    "polyNN(data_std,hidden_count=10)\n",
    "polyNN(data_std,hidden_count=50)\n",
    "polyNN(data_std,hidden_count=100)\n",
    "polyNN(data_std,hidden_count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was not overfitting to begin with, which makes the use of dropout in this case a *bad* idea\n",
    "&mdash;it increases in-sample error but does nothing to reduce out-of-sample error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Speed Things Up\n",
    "\n",
    "Due to its complexity, neural network trains a lot slower than the other techniques we have covered previously. To speed up training, we can ask PyTorch to go through more samples before updating the model's parameters by specifying a larger ```batch_size```. Doing so allows PyTorch to make better use of the CPU's parallel processing capabitilies.\n",
    "\n",
    "We previously set the default batch size to 32. We will try 128 instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = \n",
    "\n",
    "# Run training again\n",
    "polyNN(data_std,hidden_count=1,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=10,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=50,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=100,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=500,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding the number of epochs constant, what you should see with a larger batch size is faster training but also larger error. The latter is due to the fact that we are updating the parameters less often, resulting in slower learn. This can be countered by increasing the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Running Model on GPU\n",
    "\n",
    "If you have a GPU in your computer, you can now turn it on to see how much it speeds up the process of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyNN(data,hidden_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a GPU you can take advantage of its high number of core count by setting a much higher batch size, such as 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "polyNN(data,hidden_count=1,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=10,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=50,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=100,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=500,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compensate for the less frequent update, we can increase the number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epochs = 600\n",
    "polyNN(data,hidden_count=1,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=10,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=50,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=100,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=500,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Reducing Boilerplate Code\n",
    "\n",
    "Let us go back to the main differences between PyTorch and Keras/Tensorflow:\n",
    "- The model structure is defined within a subclass of `torch.nn.Module`.\n",
    "- You have to specify&mdash;i.e. code&mdash;what happen during the forward pass.\n",
    "- Pure PyTorch also requires you to code the training loop, as well as what happens during\n",
    "    validation, testing and inference. These can be replaced by trainers from libraries \n",
    "    such as [pytorch-accelerated](https://pytorch-accelerated.readthedocs.io/en/latest/)\n",
    "    or [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/).\n",
    "- Data needs to be manually placed in the right device. This can be automatically handled\n",
    "    by Hugging Face's [Accelerate](https://github.com/huggingface/accelerate) library.\n",
    "    \n",
    "We will now go through the third-party libraries mentioned above.\n",
    "\n",
    "### Hugging Face Accelerate\n",
    "\n",
    "`accelerate` removes the need to manually move our model and data to the right device. \n",
    "The benefit of doing so is not huge when we are running our model on a single GPU,\n",
    "so the primary intended usage is multi-GPU training.\n",
    "\n",
    "To use `accelerate`, simply add the following lines:\n",
    "\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Load model, build model and choose optimizer here\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "# Train your model here\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Accelerate\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Load model, build model and choose optimizer here\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=10)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch-Accelerated\n",
    "\n",
    "`pytorch_accelerated` builds on top of `accelerate` and offers a `Trainer` class\n",
    "to do the job of the training and evaluation loops. There are also callbacks for \n",
    "tasks such as logging and early stopping. This makes the usage of `pytorch_accelerated`\n",
    "very similar to Keras on Tensorflow.\n",
    "\n",
    "`pytorch_accelerated` requires PyTorch *datasets* instead of dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyToDataset(X,y,batch_size=32):\n",
    "    # Transform numpy array to torch tensor\n",
    "    tensor_X = Tensor(X)\n",
    "    tensor_y = Tensor(y)\n",
    "\n",
    "    # create datset\n",
    "    return TensorDataset(tensor_X,tensor_y) \n",
    "\n",
    "ds_train = numpyToDataset(X_train,y_train)\n",
    "ds_test = numpyToDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the model, loss function ,optimizer and callbacks to `Trainer`.\n",
    "Training is initialized by `Trainer.train()` and evaluation with `Trianer.evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch-accelerated\n",
    "from pytorch_accelerated import Trainer\n",
    "from pytorch_accelerated.callbacks import *\n",
    "\n",
    "# Create the model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Set the loss function, optimizer and number of epochs\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Callbacks. The first five are included by default.\n",
    "callbacks = [MoveModulesToDeviceCallback, \n",
    "             TerminateOnNaNCallback, \n",
    "             PrintProgressCallback, \n",
    "             ProgressBarCallback, \n",
    "             LogMetricsCallback,\n",
    "             EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    "# Set up pytorch-accelerated trainer\n",
    "trainer = Trainer(\n",
    "            model,\n",
    "            loss_func=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            callbacks=callbacks\n",
    "            )\n",
    "\n",
    "# Train the model\n",
    "trainer.train(\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_test,\n",
    "        num_epochs=10,\n",
    "        per_device_batch_size=32,\n",
    "        )\n",
    "\n",
    "# Evaluate\n",
    "trainer.evaluate(\n",
    "    dataset=ds_test,\n",
    "    per_device_batch_size=64,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
