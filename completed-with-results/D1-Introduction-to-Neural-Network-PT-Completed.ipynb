{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Network (PyTorch)\n",
    "\n",
    "Version: 2024-8-6\n",
    "\n",
    "The widespread adoption of artificial intelligence in recent years has been largely driven by advancement in neural networks. \n",
    "\n",
    "Neural network is fundamentally numeric computation, so any software with decent numeric computation capabilities can be used to construct and train a neural network. That said, while in theory you can construct a neural network in Excel, in practice it will be very troublesome since Excel is not designed with neural network in mind. Modern neural network applications have consolidated around three platforms:\n",
    "- [Tensorflow](https://www.tensorflow.org/) from Google.\n",
    "- [Flax](https://github.com/google/flax), also from Google.\n",
    "- [PyTorch](http://pytorch.org/), originally from Meta but now managed by an independent foundation.\n",
    "\n",
    "At the lowest level, these platforms are essentially NumPy with the ability to run on GPUs. \n",
    "We do not want to write the basic building blocks of neural networks from scratch, however\n",
    "if we are just trying to learn how they work.\n",
    "Therefore in this course, we will focus on two types of components that build on top of these platforms:\n",
    "\n",
    "1. **High-level API for constructing neural network**: \n",
    "    [`keras`](https://keras.io/) of Tensorflow and and \n",
    "    [`nn.Module`](https://pytorch.org/docs/stable/nn.html) of PyTorch \n",
    "    provides ready-to-use building blocks for the construction of neural networks.\n",
    "2. **Libraries that provides access to fully-trained models**. \n",
    "    The most prominent examples are Hugging Face's [Transformers](https://huggingface.co/docs/transformers/index) \n",
    "    and [fastai](https://github.com/fastai/fastai).\n",
    "\n",
    "In this notebook we will use PyTorch, which is currently the platform of choice for research. \n",
    "There is a separate notebook on the same topic that uses Keras on Tensorflow instead.\n",
    "\n",
    "<img src=\"https://scrp.econ.cuhk.edu.hk/workshops/ai/images/nn_libraries_2024.png\" width=\"80%\">\n",
    "\n",
    "## A. PyTorch vs Keras-Tensorflow\n",
    "\n",
    "If you are familiar with how neural networks are constructed in Keras,\n",
    "there is no exact equivalent on PyTorch. \n",
    "The main differences are as follows:\n",
    "- The model structure is defined within a subclass of `torch.nn.Module`.\n",
    "- You have to specify&mdash;i.e. code&mdash;what happen during the forward pass.\n",
    "- Pure PyTorch also requires you to code the training loop, as well as what happens during\n",
    "    validation, testing and inference. These can be replaced by trainers from libraries \n",
    "    such as [pytorch-accelerated](https://pytorch-accelerated.readthedocs.io/en/latest/),\n",
    "  [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/),\n",
    "  and most recently, [Keras 3](https://keras.io/keras_3/).\n",
    "- Data needs to be manually placed in the right device. This can be automatically handled\n",
    "    by Hugging Face's [Accelerate](https://github.com/huggingface/accelerate) library.\n",
    "\n",
    "These differences make PyTorch a bit harder to use, but you gain more flexibility as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will first disable the server's GPU so that everything runs on its CPU. Later we will turn it back on to see how much speed up we can get. This setting has no effect if you do not have a (Nvidia) GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: Binary Neural Network Classifier\n",
    "\n",
    "As a first example, we will train a neural network to the following classification task:\n",
    "\n",
    "|$y$|$x_1$|$x_2$|\n",
    "|-|-|-|\n",
    "|0|1|2|\n",
    "|1|0|5|\n",
    "\n",
    "with $y$ being $1 - x_1$ and $x_2$ being just an irelevant random number.\n",
    "\n",
    "To be clear: there is absolutely no need to use neural network for such as simple task. A simpler model such as logit will train a lot faster and potentially with better accuracy.\n",
    "\n",
    "We first load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Generate 2000 samples. [1,0] -> 0, [0,1] -> 1\n",
    "X = np.repeat([[1,0]], 1000, axis=0)\n",
    "y = np.repeat([[0]], 1000, axis=0)\n",
    "X = np.append(X,np.repeat([[0,1]], 1000, axis=0),axis=0)\n",
    "y = np.append(y,np.repeat([[1]], 1000, axis=0),axis=0)\n",
    "\n",
    "#Shuffle and split data into train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader\n",
    "\n",
    "Unlike Keras, NumPy arrays cannot be directly provided to PyTorch models. \n",
    "We will instead do the following:\n",
    "1. Create PyTorch tensors from NumPy arrays.\n",
    "2. Create a PyTorch Dataset from the tensors.\n",
    "3. Create a PyTorch DataLoader from the Dataset.\n",
    "\n",
    "Note that Tensorflow does have a similar data loading structure.\n",
    "It is just that Keras provides a simplier interface while PyTorch does not.\n",
    "\n",
    "Because we have to do this for every dataset we use, \n",
    "we will write a function the takes NumPy arrays and return a PyTorch Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def numpyToDataLoader(X,y,batch_size=32):\n",
    "    # Transform numpy array to torch tensor\n",
    "    tensor_X = Tensor(X)\n",
    "    tensor_y = Tensor(y)\n",
    "\n",
    "    # create datset and dataloader\n",
    "    dataset = TensorDataset(tensor_X,tensor_y) \n",
    "    return DataLoader(dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply this function to our train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = numpyToDataLoader(X_train,y_train)\n",
    "dl_test = numpyToDataLoader(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Neural Network\n",
    "\n",
    "We will construct a neural network classifier for this task. \n",
    "\n",
    "A neural network model is made up of multiple layers. The simpliest model would have three layers:\n",
    "- An *input layer*. This layer specify the nature of the input data. In this example, we only need to tell Keras that we have two variables to input.\n",
    "- A *hidden layer*. This layer contains neuron(s) that process the input data.\n",
    "- An *ouput layer*. The neurons in this layer process the output from the hidden layer and generate predictions. This layer contains as many neurons as the number of target variables we try to predict. \n",
    "\n",
    "Below is the simplest neural network one can come up with, with only one hidden neuron. The neuron computes the following function:\n",
    "}\n",
    "$$\n",
    "F \\left( b + \\sum\\nolimits_{i}{w_{i}x_{i}} \\right)\n",
    "$$\n",
    "\n",
    "where $x_i$ are inputs, b the intercept (called *bias* in machine learning), $w_i$ coefficients (called *weights*) and $F$ is an *activation function*. In this example we will use the logistic function (also called the *sigmoid function*) as the activation function:\n",
    "\n",
    "$$\n",
    "F(z) = \\frac{e^z}{1+e^z}\n",
    "$$\n",
    "\n",
    "So the neuron is essentially a logit regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (logit): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, cuda, optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Your neural network needs to be implemented in a subclass\n",
    "# of torch.nn.Module\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.logit(x)\n",
    "\n",
    "# Create the model and transfer it to the chosen device\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "PyTorch has no trainer build in, so we have to write a training loop that does the following:\n",
    "1. Loop through each epoch.\n",
    "2. Within each epoch, loop through each mini-batch.\n",
    "3. Within each mini-batch:\n",
    "    1. Compute loss.\n",
    "    2. Compute gradients.\n",
    "    3. Update parameters.\n",
    "\n",
    "The process requires us to specify the loss function and optimizer, which we will provide\n",
    "later. Additionally, we also need to write code to keep track of progress.\n",
    "\n",
    "Since we are going to train multiple models, we put the loop in a function so that we can \n",
    "reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epochs=5, quiet=False):\n",
    "    # This sets the model into training mode\n",
    "    # Does not actually train the model\n",
    "    model.train()\n",
    "\n",
    "    # Double loop: epoch - mini-batch\n",
    "    for i in range(epochs):\n",
    "        # Create an empty list to store mini-batch loss in this epoch\n",
    "        loss_list = []\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Transfer mini-batch to chosen device\n",
    "            Xb = X.to(device)\n",
    "            yb = y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(Xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad() # Reset gradients to zero\n",
    "            loss.backward()       # Compute the gradients\n",
    "            optimizer.step()      # Update parameters based on the chosen optimizer\n",
    "\n",
    "            # Append loss to loss list\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        if not quiet:\n",
    "            # Display overall loss\n",
    "            loss_overall = np.mean(loss_list)\n",
    "            print(f\"loss: {loss_overall:>7f}  epoch: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model by calling the `train` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.661169  epoch: 0\n",
      "loss: 0.655520  epoch: 1\n",
      "loss: 0.649348  epoch: 2\n",
      "loss: 0.642585  epoch: 3\n",
      "loss: 0.635235  epoch: 4\n",
      "loss: 0.627316  epoch: 5\n",
      "loss: 0.618859  epoch: 6\n",
      "loss: 0.609906  epoch: 7\n",
      "loss: 0.600506  epoch: 8\n",
      "loss: 0.590714  epoch: 9\n"
     ]
    }
   ],
   "source": [
    "# Set the loss function, optimizer and number of epochs\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "To evaluate the model, we have a loop similar to the training loop,\n",
    "but without going through multiple epochs and without updating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5846789479255676"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Transfer mini-batch to chosen device\n",
    "        Xb = X.to(device)\n",
    "        yb = y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(Xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        # Append loss to loss list\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    # Overall loss\n",
    "    loss_overall = np.mean(loss_list)\n",
    "    return loss_overall\n",
    "    \n",
    "test(dl_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike OLS, a neural network's performance could vary across runs. Run the code a few more times and see how the performance vary.\n",
    "\n",
    "### Inference\n",
    "\n",
    "We can make prediction (this is called *inference* in machine learning) with yet another loop,\n",
    "this time without computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5206432], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "x = np.array([[0,1]])\n",
    "tensor_X = Tensor(x)\n",
    "dataset = TensorDataset(tensor_X) \n",
    "dl_pred = DataLoader(dataset)\n",
    "\n",
    "# List to save prediction\n",
    "pred_list = []\n",
    "\n",
    "# Inference loop\n",
    "for batch, (X) in enumerate(dl_pred):\n",
    "    # Transfer mini-batch to chosen device\n",
    "    Xb = X[0].to(device)\n",
    "    \n",
    "    # Compute prediction\n",
    "    pred = model(Xb)\n",
    "    pred_list.append(pred.detach().numpy())\n",
    "\n",
    "# The combined array of predictions\n",
    "np.asarray(pred_list).flatten()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "Training neural network models are time consuming, so we usually want to save \n",
    "trained models for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# Load\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Activations\n",
    "\n",
    "Different activation can have profound impact on model performance. Besides ```nn.Sigmoid```, which is just a different name for the logistic function, there are other activation function such as ```nn,Tanh``` and ```nn.ReLU```. *ReLU*, which stands for **RE**ctified **L**inear **U**nit, is a particular common choice due to its good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (logit): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=1, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "loss: 0.749377  epoch: 0\n",
      "loss: 0.732456  epoch: 1\n",
      "loss: 0.717646  epoch: 2\n",
      "loss: 0.709993  epoch: 3\n",
      "loss: 0.707176  epoch: 4\n",
      "loss: 0.704789  epoch: 5\n",
      "loss: 0.702770  epoch: 6\n",
      "loss: 0.701069  epoch: 7\n",
      "loss: 0.699641  epoch: 8\n",
      "loss: 0.698448  epoch: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6993739865720272"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use ReLU instead of sigmoid in the hidden layer\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Linear(2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "\n",
    "# Create the model and transfer it to the chosen device\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=10)\n",
    "test(dl_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is ReLU preferred over the logistic function? Let us take a look at the shape of each function:\n",
    "\n",
    "<img src=\"https://scrp.econ.cuhk.edu.hk/workshops/ai/images/logistic_v_relu.png\">\n",
    "\n",
    "The most prominent feature of the logistic function is that it is bounded between 0 and 1. This means it is virtually flat for very large or very small input values, and flat means small gradient. As gradient descent relies on gradient to learn, small gradient implies slow learning. ReLU avoids this issue by being linear above zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epochs=5, quiet=False):\n",
    "    # This sets the model into training mode\n",
    "    # Does not actually train the model\n",
    "    model.train()\n",
    "\n",
    "    # Double loop: epoch - mini-batch\n",
    "    for i in range(epochs):\n",
    "        # Create an empty list to store mini-batch loss in this epoch\n",
    "        loss_list = []\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Transfer mini-batch to chosen device\n",
    "            Xb = X.to(device)\n",
    "            yb = y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(Xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad() # Reset gradients to zero\n",
    "            loss.backward()       # Compute the gradients\n",
    "            optimizer.step()      # Update parameters based on the chosen optimizer\n",
    "\n",
    "            # Append loss to loss list\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        if not quiet:\n",
    "            # Display overall loss\n",
    "            loss_overall = np.mean(loss_list)\n",
    "            print(f\"loss: {loss_overall:>7f}  epoch: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Dropout\n",
    "\n",
    "As neural networks are highly flexible, they can easily overfit. Dropout is a regularization technique that works by randomly setting the outputs of some neurons to zero, thereby forcing the network to not rely too much on a specific neurons or feature. The function below added a 50% dropout to the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Linear(2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "\n",
    "# Create the model and transfer it to the chosen device\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=10)\n",
    "test(dl_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Neural Network Regression\n",
    "\n",
    "Next we are going use a neural network in a regression task. The true data generating process (DGP) is as follows:\n",
    "\n",
    "$$\n",
    "y = x^5 -2x^3 + 6x^2 + 10x - 5\n",
    "$$\n",
    "\n",
    "The model does not know the true DGP, so it needs to figure out the relationship between $y$ and $x$ from the data.\n",
    "\n",
    "First we generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 1000 samples\n",
    "X = np.random.rand(1000,1)\n",
    "y = X**5 - 2*X**3 + 6*X**2 + 10*X - 5\n",
    "\n",
    "#Shuffle and split data into train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "dl_train = numpyToDataLoader(X_train,y_train)\n",
    "dl_test = numpyToDataLoader(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNReg(\n",
      "  (reg): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "loss: 20.172354  epoch: 0\n",
      "loss: 17.428630  epoch: 1\n",
      "loss: 15.324854  epoch: 2\n",
      "loss: 13.545914  epoch: 3\n",
      "loss: 11.888292  epoch: 4\n",
      "loss: 10.256346  epoch: 5\n",
      "loss: 8.645241  epoch: 6\n",
      "loss: 7.101048  epoch: 7\n",
      "loss: 5.680941  epoch: 8\n",
      "loss: 4.421155  epoch: 9\n",
      "loss: 3.341390  epoch: 10\n",
      "loss: 2.446793  epoch: 11\n",
      "loss: 1.734956  epoch: 12\n",
      "loss: 1.191851  epoch: 13\n",
      "loss: 0.795367  epoch: 14\n",
      "loss: 0.518490  epoch: 15\n",
      "loss: 0.334360  epoch: 16\n",
      "loss: 0.217778  epoch: 17\n",
      "loss: 0.146881  epoch: 18\n",
      "loss: 0.106868  epoch: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0843152878805995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single hidden layer with 100 neurons\n",
    "class NNReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.reg(x)\n",
    "        \n",
    "# Create the model and transfer it to the chosen device\n",
    "model = NNReg().to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=20)\n",
    "test(dl_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Model Construct Modifiable\n",
    "\n",
    "We can add arguments to the `__init__` method of our subclass of `nn.Module`, \n",
    "allowing us to create models with different settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer with variable hidden neurons and activation\n",
    "class NNReg(nn.Module):\n",
    "    def __init__(self,hidden_count=100,activation=nn.ReLU()):\n",
    "        super().__init__()        \n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(1, hidden_count),\n",
    "            activation,\n",
    "            nn.Linear(hidden_count, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.reg(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of code repetition outside of the subclass.\n",
    "We will enclose them in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def polyNN(data, \n",
    "           epochs=200, \n",
    "           batch_size=32,\n",
    "           **kwargs):\n",
    "    \n",
    "    # Record the start time\n",
    "    start = time.time()    \n",
    "    \n",
    "    # Unpack the data\n",
    "    X_train, X_test, y_train, y_test = data    \n",
    "\n",
    "    # Convert data to PyTorch tensor\n",
    "    dl_train = numpyToDataLoader(X_train,y_train,batch_size=batch_size)\n",
    "    dl_test = numpyToDataLoader(X_test,y_test,batch_size=batch_size)    \n",
    "    \n",
    "    # Create the model and transfer it to the chosen device\n",
    "    model = NNReg(**kwargs).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Train and the model\n",
    "    train(dl_train, model, loss_fn, optimizer, epochs, quiet=True)\n",
    "    \n",
    "    # Collect and display info\n",
    "    loss_tr = round(test(dl_train, model, loss_fn),4)\n",
    "    loss_te = round(test(dl_test, model, loss_fn),4)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    elapsed = round(time.time() - start,2)  \n",
    "    \n",
    "    print(\"Hidden count:\",str(kwargs['hidden_count']).ljust(5),\n",
    "          \"Parameters:\",str(param_count).ljust(6),\n",
    "          \"loss (train,test):\",str(loss_tr).ljust(7),str(loss_te).ljust(7),\n",
    "          \"Time:\",str(elapsed)+\"s\",\n",
    "         )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily try out different settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden count: 1     Parameters: 4      loss (train,test): 18.0947 17.3042 Time: 2.35s\n",
      "Hidden count: 10    Parameters: 31     loss (train,test): 0.0188  0.0171  Time: 2.39s\n",
      "Hidden count: 50    Parameters: 151    loss (train,test): 0.0021  0.002   Time: 2.39s\n",
      "Hidden count: 100   Parameters: 301    loss (train,test): 0.0018  0.0017  Time: 2.43s\n",
      "Hidden count: 500   Parameters: 1501   loss (train,test): 0.0001  0.0     Time: 2.53s\n"
     ]
    }
   ],
   "source": [
    "data = train_test_split(X,y)\n",
    "\n",
    "polyNN(data,hidden_count=1)\n",
    "polyNN(data,hidden_count=10)\n",
    "polyNN(data,hidden_count=50)\n",
    "polyNN(data,hidden_count=100)\n",
    "polyNN(data,hidden_count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the universal approximation theorem in work: the more neurons we have the better the fit.\n",
    "\n",
    "One trick that can often improve performance: *standardizing* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden count: 1     Parameters: 4      loss (train,test): 5.0191  5.462   Time: 2.33s\n",
      "Hidden count: 10    Parameters: 31     loss (train,test): 0.0085  0.0092  Time: 2.37s\n",
      "Hidden count: 50    Parameters: 151    loss (train,test): 0.0003  0.0003  Time: 2.38s\n",
      "Hidden count: 100   Parameters: 301    loss (train,test): 0.0002  0.0001  Time: 2.4s\n",
      "Hidden count: 500   Parameters: 1501   loss (train,test): 0.0     0.0     Time: 2.5s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scalar = preprocessing.StandardScaler().fit(X)\n",
    "X_std = scalar.transform(X)\n",
    "\n",
    "data_std = train_test_split(X_std,y)\n",
    "\n",
    "polyNN(data_std,hidden_count=1)\n",
    "polyNN(data_std,hidden_count=10)\n",
    "polyNN(data_std,hidden_count=50)\n",
    "polyNN(data_std,hidden_count=100)\n",
    "polyNN(data_std,hidden_count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `StandardScaler` works quite well when there is only a single feature, its sensitivity to outliers makes it unsuitable for situations with mulitple highly unbalanced features. Scikit-learn offers <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\">other scalers</a> such as `RobustScaler` that might work better in those cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer with variable hidden neurons and activation\n",
    "class NNReg(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_count=100,\n",
    "                 activation=nn.ReLU(),\n",
    "                 dropout=0.5):\n",
    "        super().__init__()        \n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(1, hidden_count),\n",
    "            activation,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_count, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.reg(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden count: 1     Parameters: 4      loss (train,test): 13.0927 14.2474 Time: 2.45s\n",
      "Hidden count: 10    Parameters: 31     loss (train,test): 4.4209  4.4352  Time: 2.54s\n",
      "Hidden count: 50    Parameters: 151    loss (train,test): 0.7853  0.8639  Time: 2.66s\n",
      "Hidden count: 100   Parameters: 301    loss (train,test): 0.4571  0.3886  Time: 2.82s\n",
      "Hidden count: 500   Parameters: 1501   loss (train,test): 0.1194  0.1031  Time: 4.04s\n"
     ]
    }
   ],
   "source": [
    "polyNN(data_std,hidden_count=1)\n",
    "polyNN(data_std,hidden_count=10)\n",
    "polyNN(data_std,hidden_count=50)\n",
    "polyNN(data_std,hidden_count=100)\n",
    "polyNN(data_std,hidden_count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was not overfitting to begin with, which makes the use of dropout in this case a *bad* idea\n",
    "&mdash;it increases in-sample error but does nothing to reduce out-of-sample error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Speed Things Up\n",
    "\n",
    "Due to its complexity, neural network trains a lot slower than the other techniques we have covered previously. To speed up training, we can ask PyTorch to go through more samples before updating the model's parameters by specifying a larger ```batch_size```. Doing so allows PyTorch to make better use of the CPU's parallel processing capabitilies.\n",
    "\n",
    "We previously set the default batch size to 32. We will try 128 instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden count: 1     Parameters: 4      loss (train,test): 18.0706 18.0547 Time: 1.02s\n",
      "Hidden count: 10    Parameters: 31     loss (train,test): 3.5852  3.1028  Time: 1.05s\n",
      "Hidden count: 50    Parameters: 151    loss (train,test): 1.0047  1.1195  Time: 1.16s\n",
      "Hidden count: 100   Parameters: 301    loss (train,test): 0.5522  0.5085  Time: 1.32s\n",
      "Hidden count: 500   Parameters: 1501   loss (train,test): 0.1175  0.1314  Time: 2.77s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "polyNN(data_std,hidden_count=1,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=10,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=50,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=100,batch_size=batch_size)\n",
    "polyNN(data_std,hidden_count=500,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding the number of epochs constant, what you should see with a larger batch size is faster training but also larger error. The latter is due to the fact that we are updating the parameters less often, resulting in slower learn. This can be countered by increasing the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Running Model on GPU\n",
    "\n",
    "If you have a GPU in your computer, you can now turn it on to see how much it speeds up the process of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden count: 1     Parameters: 4      loss (train,test): 9.147   8.3436  Time: 1.8s\n"
     ]
    }
   ],
   "source": [
    "polyNN(data,hidden_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a GPU you can take advantage of its high number of core count by setting a much higher batch size, such as 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "polyNN(data,hidden_count=1,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=10,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=50,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=100,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=500,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compensate for the less frequent update, we can increase the number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epochs = 600\n",
    "polyNN(data,hidden_count=1,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=10,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=50,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=100,epochs=epochs,batch_size=batch_size)\n",
    "polyNN(data,hidden_count=500,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Reducing Boilerplate Code\n",
    "\n",
    "Let us go back to the main differences between PyTorch and Keras/Tensorflow:\n",
    "- The model structure is defined within a subclass of `torch.nn.Module`.\n",
    "- You have to specify&mdash;i.e. code&mdash;what happen during the forward pass.\n",
    "- Pure PyTorch also requires you to code the training loop, as well as what happens during\n",
    "    validation, testing and inference. These can be replaced by trainers from libraries \n",
    "    such as [pytorch-accelerated](https://pytorch-accelerated.readthedocs.io/en/latest/)\n",
    "    or [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/).\n",
    "- Data needs to be manually placed in the right device. This can be automatically handled\n",
    "    by Hugging Face's [Accelerate](https://github.com/huggingface/accelerate) library.\n",
    "    \n",
    "We will now go through the third-party libraries mentioned above.\n",
    "\n",
    "### Hugging Face Accelerate\n",
    "\n",
    "`accelerate` removes the need to manually move our model and data to the right device. \n",
    "The benefit of doing so is not huge when we are running our model on a single GPU,\n",
    "so the primary intended usage is multi-GPU training.\n",
    "\n",
    "To use `accelerate`, simply add the following lines:\n",
    "\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Load model, build model and choose optimizer here\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "# Train your model here\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.716157  epoch: 0\n",
      "loss: 0.711143  epoch: 1\n",
      "loss: 0.704331  epoch: 2\n",
      "loss: 0.691534  epoch: 3\n",
      "loss: 0.671242  epoch: 4\n",
      "loss: 0.648086  epoch: 5\n",
      "loss: 0.623930  epoch: 6\n",
      "loss: 0.598444  epoch: 7\n",
      "loss: 0.572189  epoch: 8\n",
      "loss: 0.545715  epoch: 9\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Accelerate\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epochs=5, quiet=False):\n",
    "    # Set the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # Double loop: epoch - mini-batch\n",
    "    for i in range(epochs):\n",
    "        # Create an empty list to store mini-batch loss in this epoch\n",
    "        loss_list = []\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Transfer mini-batch to chosen device\n",
    "            Xb = X\n",
    "            yb = y\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(Xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()            # Reset gradients to zero\n",
    "            accelerator.backward(loss)       # Compute the gradients\n",
    "            optimizer.step()                 # Update parameters based on the chosen optimizer\n",
    "\n",
    "            # Append loss to loss list\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        if not quiet:\n",
    "            # Display overall loss\n",
    "            loss_overall = np.mean(loss_list)\n",
    "            print(f\"loss: {loss_overall:>7f}  epoch: {i}\")\n",
    "            \n",
    "# Create the model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Set the loss function, optimizer and number of epochs\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "train(dl_train, model, loss_fn, optimizer, epochs=10)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch-Accelerated\n",
    "\n",
    "`pytorch_accelerated` builds on top of `accelerate` and offers a `Trainer` class\n",
    "to do the job of the training and evaluation loops. There are also callbacks for \n",
    "tasks such as logging and early stopping. This makes the usage of `pytorch_accelerated`\n",
    "very similar to Keras on Tensorflow.\n",
    "\n",
    "`pytorch_accelerated` requires PyTorch *datasets* instead of dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyToDataset(X,y,batch_size=32):\n",
    "    # Transform numpy array to torch tensor\n",
    "    tensor_X = Tensor(X)\n",
    "    tensor_y = Tensor(y)\n",
    "\n",
    "    # create datset\n",
    "    return TensorDataset(tensor_X,tensor_y) \n",
    "\n",
    "ds_train = numpyToDataset(X_train,y_train)\n",
    "ds_test = numpyToDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the model, loss function ,optimizer and callbacks to `Trainer`.\n",
    "Training is initialized by `Trainer.train()` and evaluation with `Trianer.evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training run\n",
      "\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 77.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.6853388880093892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.6693557024002075\n",
      "\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 78.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.6664122145970662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 29.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.6505767970085145\n",
      "\n",
      "Improvement of 0.018778905391693024 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 79.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.6455859565734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 28.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.629736711025238\n",
      "\n",
      "Improvement of 0.02084008598327647 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 71.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.6227470517158509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.607305875301361\n",
      "\n",
      "Improvement of 0.02243083572387694 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 77.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.598303573290507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 27.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.5834183950424194\n",
      "\n",
      "Improvement of 0.023887480258941607 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 78.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.5727102936108907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 29.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.558530059337616\n",
      "\n",
      "Improvement of 0.02488833570480342 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 75.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.54671204773585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 28.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.5339872341156006\n",
      "\n",
      "Improvement of 0.024542825222015452 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 77.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.5211414604187011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.5094717569351196\n",
      "\n",
      "Improvement of 0.02451547718048097 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 79.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.49604988980293274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 29.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.48531448531150817\n",
      "\n",
      "Improvement of 0.02415727162361142 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 75.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss_epoch: 0.4714843764305115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 28.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_loss_epoch: 0.4617819027900696\n",
      "\n",
      "Improvement of 0.023532582521438583 observed, resetting counter. \n",
      "Early stopping counter: 0/3\n",
      "Finishing training run\n",
      "\n",
      "Starting evaluation run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluation_loss: 0.4617819058895111\n",
      "Finishing evaluation run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pytorch-accelerated\n",
    "from pytorch_accelerated import Trainer\n",
    "from pytorch_accelerated.callbacks import *\n",
    "\n",
    "# Create the model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Set the loss function, optimizer and number of epochs\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Callbacks. The first five are included by default.\n",
    "callbacks = [MoveModulesToDeviceCallback, \n",
    "             TerminateOnNaNCallback, \n",
    "             PrintProgressCallback, \n",
    "             ProgressBarCallback, \n",
    "             LogMetricsCallback,\n",
    "             EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    "# Set up pytorch-accelerated trainer\n",
    "trainer = Trainer(\n",
    "            model,\n",
    "            loss_func=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            callbacks=callbacks\n",
    "            )\n",
    "\n",
    "# Train the model\n",
    "trainer.train(\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_test,\n",
    "        num_epochs=10,\n",
    "        per_device_batch_size=32,\n",
    "        )\n",
    "\n",
    "# Evaluate\n",
    "trainer.evaluate(\n",
    "    dataset=ds_test,\n",
    "    per_device_batch_size=64,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
